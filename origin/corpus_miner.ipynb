{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXtf-0zWNwin"
      },
      "source": [
        "# General description of the solution\n",
        "\n",
        "TDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fY9HerUOPRT"
      },
      "source": [
        "## Preparation\n",
        "### Dependency installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f3Sf1sqOKTq",
        "outputId": "0b421cf4-e24b-435a-b943-bc88cde52a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting levenshtein\n",
            "  Downloading Levenshtein-0.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110 kB 31.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting rapidfuzz<1.9,>=1.8.2\n",
            "  Downloading rapidfuzz-1.8.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (854 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 854 kB 21.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=e1cb52e4aef7da945fc88fe816be8691edfc6dc3b694c67fd7b2c10d1e69069d\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, rapidfuzz, levenshtein, feedparser\n",
            "Successfully installed feedparser-6.0.8 levenshtein-0.16.0 rapidfuzz-1.8.3 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy feedparser levenshtein pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b18fBKx4HVdR"
      },
      "outputs": [],
      "source": [
        "# !pip install textract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu9j9cG-Pf4t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import textract\n",
        "import requests\n",
        "import shutil\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfYXEXykN3c1"
      },
      "source": [
        "### Data location\n",
        "\n",
        "Below we setup a working folder and a data source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAAkijkfNr_8"
      },
      "outputs": [],
      "source": [
        "# this line is specific for execution at Google Drive\n",
        "def mount_gdrive():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxMUV1-tP-jV"
      },
      "outputs": [],
      "source": [
        "def init_folders():\n",
        "    print(\"Initializing folder for a pipeline\")\n",
        "    import os\n",
        "    global DATA_SOURCE_FOLDER\n",
        "    global RESULTS_FOLDER \n",
        "    if not os.path.exists(DATA_SOURCE_FOLDER): os.mkdir(DATA_SOURCE_FOLDER)\n",
        "    if not os.path.exists(RESULTS_FOLDER): os.mkdir(RESULTS_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSd5rsYYRpoh"
      },
      "source": [
        "## Reading the source files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uspt4np_Ro8B"
      },
      "outputs": [],
      "source": [
        "def _data_for_year(year):\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    filename = os.path.join(DATA_SOURCE_FOLDER, f\"{year}.xlsx\")\n",
        "    xls = pd.ExcelFile(filename)\n",
        "    sheet = xls.parse(f\"{year}\")\n",
        "    result = []\n",
        "    for row in sheet[sheet.columns[1]]:\n",
        "        p1 = row.find(',\"')\n",
        "        p2 = row.find('\"', p1 + 2)\n",
        "        authors = row[:p1].split(', ')\n",
        "        \n",
        "        # HOTFIX for existing table\n",
        "        if str(year) == '2018':\n",
        "            p1 = row.find(',\"', p2)\n",
        "            p2 = row.find('\"', p1 + 2)\n",
        "        \n",
        "        title = row[p1+2:p2]\n",
        "        result.append({'title': title, 'authors': authors, 'data': str(year)})\n",
        "    return result\n",
        "\n",
        "\n",
        "def load_all_scopus_titles(_range):\n",
        "    result = {}\n",
        "    for year in _range:\n",
        "        result[str(year)] = _data_for_year(year)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFaI46YZZwly"
      },
      "source": [
        "### Saving the dump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aiHPNzlZvRp"
      },
      "outputs": [],
      "source": [
        "import pickle, os\n",
        "\n",
        "def save_index(ind, foldername, prefix=None):\n",
        "    global RESULTS_FOLDER\n",
        "    if prefix is None:\n",
        "        prefix = RESULTS_FOLDER\n",
        "    folder = os.path.join(prefix, foldername)\n",
        "    if not os.path.exists(folder):\n",
        "        os.mkdir(folder)\n",
        "    for year in ind:\n",
        "        with open(os.path.join(folder, str(year)), 'wb') as f:\n",
        "            pickle.dump(ind[year], f)\n",
        "\n",
        "\n",
        "def load_index(foldername, \n",
        "         years=[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020], \n",
        "         prefix=None,\n",
        "         with_raw_texts=False): \n",
        "    from tqdm import tqdm   \n",
        "    global RESULTS_FOLDER\n",
        "    if prefix is None:\n",
        "        prefix = RESULTS_FOLDER\n",
        "    result = {}\n",
        "    for year in years:\n",
        "        filename = os.path.join(prefix, foldername, str(year))\n",
        "        print(\"Checking filename:\", filename)\n",
        "        if os.path.exists(filename):\n",
        "            print(\"Loading year\", year)\n",
        "            with open(filename, 'rb') as f:\n",
        "                result[str(year)] = pickle.load(f)\n",
        "            # hack\n",
        "            for item in tqdm(result[str(year)]):\n",
        "                if os.path.exists(item['textfile']) and 'raw' not in item:\n",
        "                    with open(item['textfile'], 'r') as f:\n",
        "                        item['raw'] = f.read()\n",
        "            print(f\"Loaded for year {year}: {len(result[str(year)])} items\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRJBGd9jWu4M"
      },
      "source": [
        "## Find Arxiv papers relevant to our titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XukdmNzgWtmb"
      },
      "outputs": [],
      "source": [
        "def _get_relevant(title, n=100):\n",
        "    api = f\"http://export.arxiv.org/api/query?max_results={n}&search_query=\"\n",
        "    import time\n",
        "    import feedparser\n",
        "\n",
        "    # 3 seconds delay is due to arxiv API requirements\n",
        "    time.sleep(3)\n",
        "    return feedparser.parse(api + title.replace(' ', '+'))\n",
        "\n",
        "\n",
        "def _to_papers(feed):\n",
        "    result = []\n",
        "    for e in feed[\"entries\"]:\n",
        "        id = e['id'][21:].replace('/', '_')\n",
        "        page = e['id']\n",
        "        year = e['published'].split('-')[0]\n",
        "        pdfurl = [l['href'] for l in e['links'] if l['type'] == 'application/pdf'][0]\n",
        "        title = e['title'].replace('\\n', '').replace('  ', ' ')\n",
        "        authors = [a['name'] for a in e.authors]\n",
        "        result.append({\n",
        "            'id': id,\n",
        "            'url': page,\n",
        "            'year': year,\n",
        "            'pdfurl': pdfurl,\n",
        "            'title': title,\n",
        "            'authors': authors\n",
        "        })\n",
        "    return result\n",
        "\n",
        "\n",
        "def _filter_relevant_papers(feed, item, LD=10, IOU=.01):\n",
        "    import itertools\n",
        "    import Levenshtein\n",
        "    title = item['title'].lower()\n",
        "\n",
        "    def author_set(authors):\n",
        "        return set([name.lower() for name \n",
        "                    in itertools.chain(*[i.split() for i in authors]) if '.' not in name])\n",
        "\n",
        "    s1 = author_set(item['authors'])\n",
        "    result = []\n",
        "    for paper in feed:\n",
        "        dist = Levenshtein.distance(paper['title'].lower(), title)\n",
        "        s2 = author_set(paper['authors'])\n",
        "        iou = len(set.intersection(s1, s2)) / len(set.union(s1, s2))\n",
        "        if iou >= IOU and dist <= LD:\n",
        "            paper['source'] = item['title']\n",
        "            result.append(paper)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def collect_paper_meta(sources, idx_name='index_filtered'):\n",
        "    from tqdm import tqdm\n",
        "    result = load_index(idx_name)\n",
        "    for year in sources:\n",
        "        print(\"Collecting metainfo for year\", year)\n",
        "        if str(year) in result:\n",
        "            continue\n",
        "        pidx = load_index(idx_name + \"_parts\", [year])\n",
        "        result[year] = pidx[str(year)] if str(year) in pidx else []\n",
        "        source_titles = [item['source'] for item in result[year]]\n",
        "        for i, paper in enumerate(tqdm(sources[year])):\n",
        "            # these papers is already loaded for this year in parts\n",
        "            if paper['title'] in source_titles:\n",
        "                continue\n",
        "            # this line is the most important, as it\n",
        "            feed = _get_relevant(paper['title'])\n",
        "            candidates = _to_papers(feed)\n",
        "            filtered = _filter_relevant_papers(candidates, paper)\n",
        "            result[year] += filtered\n",
        "            if (i + 1) % 100 == 0:\n",
        "                save_index({year: result[year]}, idx_name + \"_parts\")\n",
        "        save_index(result, idx_name)\n",
        "        print(\"index for year\", year, \"saved to\", idx_name)\n",
        "    return result\n",
        "\n",
        "\n",
        "def reorder_by_year(index):\n",
        "    result = {}\n",
        "    for year in index:\n",
        "        for item in index[year]:\n",
        "            if item['year'] not in result:\n",
        "                result[item['year']] = []\n",
        "            result[item['year']].append(item) \n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPN_BLXheuGp"
      },
      "source": [
        "## Report papers collected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LkIg68Mjiet"
      },
      "outputs": [],
      "source": [
        "def report_index_size(index, filter_by_year=True):\n",
        "    header = f\" year\\t|\\titems\\t|\\t=year\"\n",
        "    print(header)\n",
        "    print(\"-------------------------------------\")\n",
        "    for year in sorted(index.keys()):\n",
        "        c = len(index[year])\n",
        "        if filter_by_year:\n",
        "            cf = len([item for item in index[year] if int(year) - int(item['year']) < 2])\n",
        "        else:\n",
        "            cf = c\n",
        "        s = f\" {year}\\t|\\t{c:5}\\t|\\t{cf:5}\"\n",
        "        print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnesJm-XhsV4"
      },
      "source": [
        "## Download files\n",
        "\n",
        "We will download only paper which are matching all criteria, including exact year match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaKYW9vznGVa"
      },
      "outputs": [],
      "source": [
        "def download_papers(index, folder=\"pdf\", strict_year=False, delay=3):\n",
        "    from tqdm import tqdm\n",
        "    import requests\n",
        "    import shutil\n",
        "    import time\n",
        "    import urllib\n",
        "\n",
        "    global RESULTS_FOLDER\n",
        "\n",
        "    fullfolder = os.path.join(RESULTS_FOLDER, folder)\n",
        "    if not os.path.exists(fullfolder):\n",
        "        os.mkdir(fullfolder)\n",
        "    for year in index:\n",
        "        print(f\"Downloading {year} year\")\n",
        "        yearfolder = os.path.join(fullfolder, str(year))\n",
        "        if not os.path.exists(yearfolder):\n",
        "            os.mkdir(yearfolder)\n",
        "\n",
        "        for item in tqdm(index[year]):\n",
        "            if strict_year and item['year'] == str(year):\n",
        "                continue\n",
        "            # + '.pdf' - hack\n",
        "            url = item['pdfurl'].replace('http:', 'https:') + '.pdf'\n",
        "            short_file = item['id'] + '.pdf'\n",
        "            filename = os.path.join(yearfolder, short_file)\n",
        "            item['pdffile'] = filename\n",
        "            if os.path.exists(filename):\n",
        "                # hack for the cases file was downloaded partially\n",
        "                if os.path.getsize(filename) > 16 * 1024:\n",
        "                    continue\n",
        "            time.sleep(delay)\n",
        "            # urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "            with requests.get(url, stream=True, allow_redirects=True) as r:\n",
        "                if str(r.status_code)[0] in '45':\n",
        "                    print(f\"Error: {r.status_code}, {r.url}\")\n",
        "                    if str(r.status_code) == '403':\n",
        "                        raise Exception(\"We are banned by arxiv :(\")\n",
        "                else:\n",
        "                    with open(filename, 'wb') as f:\n",
        "                        shutil.copyfileobj(r.raw, f, 1024 * 1024 * 5)\n",
        "                    \n",
        "        # todo remove\n",
        "        save_index({year: index[year]}, \"index_with_pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xB6km8yrDO9"
      },
      "source": [
        "## Recognize all texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9i_SI99rR_V"
      },
      "outputs": [],
      "source": [
        "def recognize_texts(index, dest_folder='txt'):\n",
        "    import os, textract\n",
        "  \n",
        "    global RESULTS_FOLDER\n",
        "\n",
        "    full_dest_folder = os.path.join(RESULTS_FOLDER, dest_folder)\n",
        "    if not os.path.exists(full_dest_folder):\n",
        "        os.mkdir(full_dest_folder)\n",
        "\n",
        "    for year in index:\n",
        "        year_dest_folder = os.path.join(RESULTS_FOLDER, dest_folder, str(year))\n",
        "        if not os.path.exists(year_dest_folder):\n",
        "            os.mkdir(year_dest_folder)\n",
        "        failed = 0\n",
        "        for item in tqdm(index[year]):\n",
        "            pdf = item['pdffile']\n",
        "            txtfile = os.path.join(year_dest_folder, pdf.split('/')[-1][:-4] + '.txt')\n",
        "            item['textfile'] = txtfile\n",
        "            if os.path.exists(txtfile):\n",
        "                continue\n",
        "            try:\n",
        "                bin = textract.process(pdf, method='pdfminer')\n",
        "            except BaseException as e:\n",
        "                print(e)\n",
        "                failed += 1\n",
        "                continue\n",
        "            text = str(bin, encoding=\"utf8\")\n",
        "            item['raw'] = text\n",
        "            with open(txtfile, 'w') as f:\n",
        "                f.write(item['raw'])\n",
        "        print(f\"Year {year} failed {failed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eko8x03GspaZ"
      },
      "source": [
        "## Prepare clean dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8fzAD3ZsuBH"
      },
      "outputs": [],
      "source": [
        "def _clean_numbers(string):\n",
        "    import re\n",
        "    string = re.sub('\\\\|\\\\d+i', '', string)\n",
        "    string = re.sub('\\\\|\\\\d+>', '', string)\n",
        "    string = re.sub('\\\\[[\\\\d+\\\\.\\\\-‚àí‚Äì\\\\s,]+\\\\]', '', string)\n",
        "    string = re.sub('\\\\(\\\\d+\\\\)', '', string)\n",
        "    string = re.sub('\\\\b\\\\d+(\\\\.\\\\d+)?%?', '', string)\n",
        "    string = string.replace('(cid:)', '')\n",
        "    string = re.sub(\"\\\\(\\\\s?\\\\)\", \"\", string)\n",
        "    string = re.sub(\"\\\\[\\\\s?\\\\]\", \"\", string)\n",
        "    return string\n",
        "\n",
        "\n",
        "def _remove_greek(string):\n",
        "    import re\n",
        "    string = re.sub(\"[ŒëŒ±ŒíŒ≤ŒìŒ≥ŒîŒ¥ŒïŒµŒñŒ∂ŒóŒ∑ŒòŒ∏ŒôŒπŒöŒ∫ŒõŒªŒúŒºŒùŒΩŒûŒæŒüŒøŒ†œÄŒ°œÅŒ£œÉœÇŒ§œÑŒ•œÖŒ¶œÜŒßœáŒ®œàŒ©œâ‚äó‚Ä†‚Üì‚Üí‚àû‚Üë‚Üì=‚Üî]+\", \"\", string)\n",
        "    return string\n",
        "\n",
        "\n",
        "def prepare_clean_dataset(index):\n",
        "    import re, os\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    tails = ['acknowledgements\\n', 'references\\n', 'bibliograpy\\n', \n",
        "         'confilicts of interest\\n', \"acknowledges support\", \"are grateful to the funding\",\n",
        "         'acknowledgements.', 'acknowledgment ‚Äì', 'we thank', 'is gratefully acknowledged', 'appendix a', 'references .',\n",
        "         'research was supported by', 'acknowledges support by', 'we acknowledge funding', 'work is supported by',\n",
        "         'was supported by the funding', 'is supported by the funding', ]\n",
        "\n",
        "    for year in index:\n",
        "        print(\"Year\", year)\n",
        "        # TODO restore partial load, but as this stage this is not critical\n",
        "        for item in tqdm(index[year]):\n",
        "            if 'raw' not in item:\n",
        "                continue\n",
        "            raw = item['raw']\n",
        "            mintail = None\n",
        "            for tail in tails:\n",
        "                tailstart = raw.lower().rfind(tail)\n",
        "                if tailstart > -1:\n",
        "                    if mintail is None:\n",
        "                        mintail = tailstart\n",
        "                    else:\n",
        "                        if tailstart > len(raw) // 2:\n",
        "                            mintail = min(tailstart, mintail)\n",
        "            if mintail is not None and mintail > -1 and mintail > len(raw) // 2:\n",
        "                raw = raw[:mintail]\n",
        "            raw = raw.replace('\\r\\n\\r\\n', ' ').replace('\\n\\n', ' ')\n",
        "            raw = '\\n'.join([line for line in raw.split('\\n') if len(line.replace(' ', '')) > 3])\n",
        "            raw = raw.replace(\"Ô¨É\", \"ffi\").replace(\"Ô¨Å\", \"fi\").replace(\"Ô¨Ç\", \"fl\").replace('Ô¨Ä', 'ff')\n",
        "            raw = _clean_numbers(raw)\n",
        "            raw = _remove_greek(raw)\n",
        "            raw = raw.replace(\"-\\n\", \"\")\n",
        "            raw = raw.replace(\"\\n\", \" \")\n",
        "            item['clean'] = raw\n",
        "            del item['raw']\n",
        "\n",
        "\n",
        "def dataset_unload(index, key, folder):\n",
        "    import os\n",
        "    global RESULTS_FOLDER\n",
        "    full_dest_folder = os.path.join(RESULTS_FOLDER, folder)\n",
        "    if not os.path.exists(full_dest_folder):\n",
        "        os.mkdir(full_dest_folder)\n",
        "\n",
        "    for year in index:\n",
        "        year_folder = os.path.join(full_dest_folder, str(year))\n",
        "        if not os.path.exists(year_folder):\n",
        "            os.mkdir(year_folder)\n",
        "        for item in index[year]:\n",
        "            if key in item:\n",
        "                # print(os.path.join(year_folder, f\"{item['id']}.txt\"))\n",
        "                with open(os.path.join(year_folder, f\"{item['id']}.txt\"), 'w') as ff:\n",
        "                    ff.write(item[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpFL9XH-uWG1"
      },
      "source": [
        "## Do some NLP magic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOGJVRJWubPY"
      },
      "outputs": [],
      "source": [
        "def get_wic(sent, nlp, context_radius=5):\n",
        "    thread = nlp(sent)\n",
        "    n = len(thread)\n",
        "    res = []\n",
        "    for i in range(n):\n",
        "        lemma = thread[i].lemma_\n",
        "        pos = thread[i].pos_\n",
        "        context = thread[max(0, i - context_radius):min(n, i + context_radius + 1)]\n",
        "        text_context = \" \".join([x.text for x in context])\n",
        "        res.append({\n",
        "            'lem': lemma,\n",
        "            'pos': pos,\n",
        "            'ctx': text_context\n",
        "        })\n",
        "    return res\n",
        "    \n",
        "\n",
        "def text_to_lemmas(text, nlp):\n",
        "    result = []\n",
        "    for sent in [s.strip() for s in text.split('.')]:\n",
        "        tokenbox = []\n",
        "        if not sent: continue\n",
        "        thread = nlp(sent)\n",
        "        for i in range(len(thread)):\n",
        "            lemma = thread[i].lemma_\n",
        "            pos = thread[i].pos_\n",
        "            text = thread[i].text\n",
        "            tokenbox.append({\n",
        "                'lem': lemma,\n",
        "                'txt': text, \n",
        "                'pos': pos\n",
        "            })\n",
        "        result.append(tokenbox)\n",
        "    return result\n",
        "\n",
        "\n",
        "def prepare_lemmas(years, index_source_folder, index_dest_folder):\n",
        "    import os\n",
        "    from tqdm import tqdm\n",
        "    import spacy\n",
        "\n",
        "    global RESULTS_FOLDER\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    for year in years:\n",
        "        year = str(year)\n",
        "        print(\"Lemmatizing year\", year)\n",
        "\n",
        "        # Hack\n",
        "        if os.path.exists(os.path.join(RESULTS_FOLDER, index_dest_folder, year)):\n",
        "            print(f\"Year {year} already processed\")\n",
        "            continue\n",
        "\n",
        "        nu_index = {year: []}\n",
        "        index = load_index(index_source_folder, years=[str(year)])\n",
        "        print(index.keys())\n",
        "        for item in tqdm(index[year]):\n",
        "            # we will write text representation of file\n",
        "            filename = f\"{year}_{item['id']}.txt\"\n",
        "            if 'clean' not in item:\n",
        "                continue\n",
        "            item[\"tokens\"] = text_to_lemmas(item['clean'], nlp)\n",
        "            del item['clean']\n",
        "            nu_index[year].append(item)\n",
        "        save_index(nu_index, index_dest_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YsNvpnIFh_U"
      },
      "source": [
        "## Count keyword stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GydoDRC0FhLD"
      },
      "outputs": [],
      "source": [
        "def keyword_stats(index_folder, years=None, allowed=['ADJ', 'ADV', 'NOUN', 'VERB']):\n",
        "    from collections import Counter\n",
        "    import itertools\n",
        "    from tqdm import tqdm\n",
        "    result = Counter()\n",
        "    if years is None: years = range(2010, 2021)\n",
        "    for year in years:\n",
        "        print(\"Counting stats in\", year)\n",
        "        year = str(year)\n",
        "        idx = load_index(index_folder, [year])\n",
        "        for item in tqdm(idx[year]):\n",
        "            if 'tokens' not in item:\n",
        "                continue\n",
        "            for sent in item['tokens']:\n",
        "                toks = [(token['lem'].lower(), token['pos']) \n",
        "                            for token \n",
        "                            in sent\n",
        "                            if token['pos'] in allowed\n",
        "                            ]\n",
        "                result.update(toks)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6HW8ofDK0-b"
      },
      "outputs": [],
      "source": [
        "def blacklist(counter):\n",
        "    result = set()\n",
        "    for key in counter:\n",
        "        for symbol in ',[]~Àú!@#$%^&*()+`\"‚Ññ;%:?*(){}/\\\\|<>\\'?√ó.‚â°‚â§¬∑‚àà‚àá‚àÜ‚Äù‚ÄúùúÖùê¥ùëù‚Ä≤¬±ŒëŒ±ŒíŒ≤ŒìŒ≥ŒîŒ¥ŒïŒµŒñŒ∂ŒóŒ∑ŒòŒ∏ŒôŒπŒöŒ∫ŒõŒªŒúŒºŒùŒΩŒûŒæŒüŒøŒ†œÄŒ°œÅŒ£œÉœÇŒ§œÑŒ•œÖŒ¶œÜŒßœáŒ®œàŒ©œâ‚äó‚Ä†‚Üì‚Üí‚àû‚Üë‚Üì=‚Üî‚àíÀÜ\\uf8f7':\n",
        "            if symbol in key[0]:\n",
        "                result.add(key)\n",
        "                break\n",
        "        if len(key[0]) == 1:\n",
        "            result.add(key)\n",
        "        if key[-1] == '-':\n",
        "            result.add(key)\n",
        "        elif key[0] in ['--', \"the\", \"of\", \"a\", 'in', 'to', 'is', 'for', 'that', \n",
        "                     'we', 'as', 'with', 'by', 'be', 'are', 'on', 'this', '‚àí', \n",
        "                     '+', 'can', 'an', 'at', 'where', 'not', 'our', 'out', 'fig', 'it',\n",
        "                     'one', 'two', 'or', 'eq', 'may', 'have', 'such', 'also', \n",
        "                     'while', 'each', 'all', 'only', 'more', 'if', 'these', \n",
        "                     'has', 'thus', 'its', 'ii', 'iii', 'vi', 'iv', 'vii', \n",
        "                     'viii', 'xi', 'ix', 'there', 'their', 'into', 'any', \n",
        "                     'through', 'so', 'they', 'under', 'now', 'per', 'could', 'can', \n",
        "                     'does', 'do', 'use', 'new', 'tree', 'four', 'some',\n",
        "                     \"was\", \"from\", \"were\", \"which\", \"but\", \"who\", \"and\", \"her\", \"them\", \"many\", \"both\", \"my\", \"after\", \"she\", \n",
        "                     \"about\", \"other\", \"his\", \"he\", \"than\", \"had\", \"when\",\n",
        "                    \"will\", \"been\", \"what\", \"would\", \"between\", \"most\", \"no\"]:\n",
        "            result.add(key)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9U484fLLpJM"
      },
      "outputs": [],
      "source": [
        "def save_stats(counter, filename, prefix=None):\n",
        "    import os\n",
        "    global RESULTS_FOLDER\n",
        "    total = sum(counter.values())\n",
        "\n",
        "    if prefix is None:\n",
        "        prefix = RESULTS_FOLDER\n",
        "\n",
        "    dest = os.path.join(prefix, filename)\n",
        "    with open(dest, 'w') as f:\n",
        "        for key, value in counter.most_common():\n",
        "            for k in key:\n",
        "                f.write(f\"{k}\\t\")\n",
        "            f.write(f\"{value}\\t{value / total:.5f}\\n\")\n",
        "\n",
        "\n",
        "def save_stats_from_list(freq, filename, prefix=None):\n",
        "    import os\n",
        "    global RESULTS_FOLDER\n",
        "\n",
        "    if prefix is None:\n",
        "        prefix = RESULTS_FOLDER\n",
        "\n",
        "    dest = os.path.join(prefix, filename)\n",
        "    common = sorted(freq, key=lambda x: -x[1])\n",
        "    with open(dest, 'w') as f:\n",
        "        for key, value in common:\n",
        "            for k in key:\n",
        "                f.write(f\"{k}\\t\")\n",
        "            f.write(f\"{value}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log2\n",
        "\n",
        "def to_freq(counter):\n",
        "    total = 0\n",
        "    for key in counter:\n",
        "        total += counter[key]\n",
        "    result = {}\n",
        "    for key in counter:\n",
        "        result[key] = counter[key] / total\n",
        "    return result\n",
        "\n",
        "\n",
        "def partial_kld(px, qx):\n",
        "    if px == 0:\n",
        "        return 0\n",
        "    elif qx == 0:\n",
        "        #TODO: word is not present\n",
        "        return 0\n",
        "\n",
        "    divergence = px * log2(px / qx)    \n",
        "    return divergence\n",
        "\n",
        "\n",
        "def KLD(p, q):\n",
        "    keys = set(p.keys()).union(q.keys())\n",
        "    divergence = 0.\n",
        "    for x in keys:\n",
        "        px = 0 if x not in p else p[x]\n",
        "        qx = 0 if x not in q else q[x]\n",
        "        divergence += partial_kld(px, qx)\n",
        "    return divergence\n",
        "    \n",
        "\n",
        "def get_from_first_top_KL_outliers(p, q, threshold, include_new=True):\n",
        "    keys = set(p.keys()).union(q.keys())\n",
        "    result = {}\n",
        "    for key in keys:\n",
        "        if key in p and key not in q:\n",
        "            if include_new:\n",
        "                result[key] = float('inf')\n",
        "        elif key in p and key in q:\n",
        "            pkld = partial_kld(p[key], q[key])\n",
        "            if pkld > threshold:\n",
        "                result[key] = pkld\n",
        "    return result"
      ],
      "metadata": {
        "id": "gzeaoF5sgyY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwkkBE2LQjCP"
      },
      "source": [
        "# Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTeBSMIlK0ps"
      },
      "outputs": [],
      "source": [
        "DATA_SOURCE_FOLDER = \"/gdrive/MyDrive/data/physics/scopus/\"\n",
        "RESULTS_FOLDER = \"/gdrive/MyDrive/data/physics/results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSL2cZpxP-mP",
        "outputId": "5e0155cc-b337-4f48-84c3-359393d57666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "Initializing folder for a pipeline\n",
            " year\t|\titems\t|\t=year\n",
            "-------------------------------------\n",
            " 2010\t|\t 1557\t|\t 1557\n",
            " 2011\t|\t 1549\t|\t 1549\n",
            " 2012\t|\t 1610\t|\t 1610\n",
            " 2013\t|\t 1660\t|\t 1660\n",
            " 2014\t|\t 1658\t|\t 1658\n",
            " 2015\t|\t 1848\t|\t 1848\n",
            " 2016\t|\t 2000\t|\t 2000\n",
            " 2017\t|\t 2000\t|\t 2000\n",
            " 2018\t|\t 1999\t|\t 1999\n",
            " 2019\t|\t 2000\t|\t 2000\n",
            " 2020\t|\t 1609\t|\t 1609\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# if at gdrive\n",
        "mount_gdrive()\n",
        "init_folders()\n",
        "references = load_all_scopus_titles(range(2010, 2021))\n",
        "report_index_size(references, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBg4wdNy7qnk"
      },
      "outputs": [],
      "source": [
        "meta = collect_paper_meta(references)\n",
        "save_index(meta, 'index_0_meta')\n",
        "report_index_size(meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I20f4FrA7w2l"
      },
      "outputs": [],
      "source": [
        "meta_reordered = reorder_by_year(meta)\n",
        "save_index(meta_reordered, \"index_1_meta_reordered\")\n",
        "report_index_size(meta_reordered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOH0BXBm-x3e"
      },
      "outputs": [],
      "source": [
        "download_papers(meta_reordered, delay=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O6gl3LGVMiF"
      },
      "outputs": [],
      "source": [
        "save_index(meta_reordered, \"index_2_with_pdf_path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzjT6i79Va8R"
      },
      "outputs": [],
      "source": [
        "recognize_texts(meta_reordered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhYHgjuXsfif"
      },
      "outputs": [],
      "source": [
        "save_index(meta_reordered, \"index_3_raw_texts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6yjtLzK7j4j"
      },
      "outputs": [],
      "source": [
        "index_reordered = load_index(\"index_3_raw_texts\", with_raw_texts=True)\n",
        "prepare_clean_dataset(index_reordered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QFg8qg7FY0q"
      },
      "outputs": [],
      "source": [
        "save_index(index_reordered, \"index_4_clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57wIl6YaHqOu"
      },
      "outputs": [],
      "source": [
        "prepare_clean_dataset(index_reordered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdDqMn4uIHax"
      },
      "outputs": [],
      "source": [
        "dataset_unload(index_reordered, 'clean', 'txt_corpus_clean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s_BtBaT12E3"
      },
      "outputs": [],
      "source": [
        "index_clean = load_index(\"index_4_clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZA6OeVf4mbl"
      },
      "outputs": [],
      "source": [
        "prepare_lemmas(range(2010, 2021), \"index_4_clean\", \"index_5_tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "202uM3FVQQKS"
      },
      "outputs": [],
      "source": [
        "load_index('index_5_tokens', years=[2020])['2020'][50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln1mGHJjFN8-",
        "outputId": "d99e6461-500a-455f-a4bc-a9c54d543074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting stats in 2011\n",
            "Checking filename: /gdrive/MyDrive/data/physics/results/index_5_tokens/2011\n",
            "Loading year 2011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 473/473 [00:00<00:00, 3364.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded for year 2011: 473 items\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 473/473 [00:01<00:00, 268.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('state', 'NOUN'), 15958), (('can', 'VERB'), 11645), (('use', 'VERB'), 8882), (('system', 'NOUN'), 8870), (('show', 'VERB'), 6743), (('where', 'ADV'), 6575), (('time', 'NOUN'), 6198), (('field', 'NOUN'), 5995), (('b', 'NOUN'), 4937), (('t', 'NOUN'), 4834)]\n"
          ]
        }
      ],
      "source": [
        "stats = keyword_stats('index_5_tokens', years=[2011])\n",
        "print(stats.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EosZWAk8HhHd"
      },
      "outputs": [],
      "source": [
        "black1 = blacklist(stats)\n",
        "for b in black1: del stats[b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udyBV1zbLbGB"
      },
      "outputs": [],
      "source": [
        "save_stats(stats, 'most_common_lemmas.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGbb8dvlOLHH"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "with open('/gdrive/MyDrive/data/physics/text_acad.txt', 'r') as f:\n",
        "    lemmas = text_to_lemmas(f.read(), nlp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_index({\"acad\": [{\"tokens\": lemmas, 'textfile' : \"\"}] }, \"academ_corpus\")\n",
        "acad_stats = keyword_stats(\"academ_corpus\", years=['acad'])\n",
        "black2 = blacklist(acad_stats)\n",
        "for b in black2: del acad_stats[b]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfjnX00MPG4c",
        "outputId": "d481bafe-9a11-493a-bf04-78c39a22a5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting stats in acad\n",
            "Checking filename: /gdrive/MyDrive/data/physics/results/academ_corpus/acad\n",
            "Loading year acad\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 4644.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded for year acad: 1 items\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uni_texts = to_freq(stats)\n",
        "uni_acad = to_freq(acad_stats)"
      ],
      "metadata": {
        "id": "90mU9p1ufwat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers = get_from_first_top_KL_outliers(uni_texts, uni_acad, 0.0001, False)\n",
        "outliers_with_new = get_from_first_top_KL_outliers(uni_texts, uni_acad, 0.0001, True)"
      ],
      "metadata": {
        "id": "ntfBa3kzkK6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_stats_from_list(list(outliers.items()), \"outliers.txt\")\n",
        "save_stats_from_list(list(outliers_with_new.items()), \"outliers_with_new.txt\")"
      ],
      "metadata": {
        "id": "qOwFkX7ZvPtE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "corpus_miner.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}